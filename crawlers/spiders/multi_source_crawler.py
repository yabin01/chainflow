from utils.base_crawler import BaseCrawler
from datetime import datetime
import time
import random

class MultiSourceCrawler(BaseCrawler):
    def __init__(self):
        super().__init__('å¤šæºçˆ¬è™«æµ‹è¯•', 'https://multi.chainflow.com')
        
        # å®šä¹‰è¦æµ‹è¯•çš„ç½‘ç«™åˆ—è¡¨
        self.websites = [
            {
                'name': 'Foresight News',
                'url': 'https://foresightnews.pro',
                'test_url': 'https://foresightnews.pro/article/list',
                'type': 'news'
            },
            {
                'name': 'é“¾æ•æ‰‹ ChainCatcher',
                'url': 'https://www.chaincatcher.com',
                'test_url': 'https://www.chaincatcher.com/',
                'type': 'news'
            },
            {
                'name': 'TechFlow',
                'url': 'https://techflowpost.com',
                'test_url': 'https://techflowpost.com/',
                'type': 'news'
            },
            {
                'name': 'Odaily',
                'url': 'https://www.odaily.news',
                'test_url': 'https://www.odaily.news/',
                'type': 'news'
            },
            {
                'name': 'å¾‹åŠ¨ BlockBeats',
                'url': 'https://www.theblockbeats.info',
                'test_url': 'https://www.theblockbeats.info/',
                'type': 'news'
            },
            {
                'name': 'PANews',
                'url': 'https://www.panewslab.com',
                'test_url': 'https://www.panewslab.com/zh/index.html',
                'type': 'news'
            },
            {
                'name': 'Decrypt',
                'url': 'https://decrypt.co',
                'test_url': 'https://decrypt.co/',
                'type': 'news'
            },
            {
                'name': 'CoinAnk',
                'url': 'https://coinank.com',
                'test_url': 'https://coinank.com/',
                'type': 'news'
            }
        ]

    def test_website_accessibility(self, website):
        """æµ‹è¯•ç½‘ç«™å¯è®¿é—®æ€§"""
        print(f"ğŸ” æµ‹è¯• {website['name']} ({website['test_url']})...")
        
        try:
            start_time = time.time()
            html_content = self.fetch_page(website['test_url'], timeout=15)
            response_time = round((time.time() - start_time) * 1000, 2)
            
            if html_content:
                # ç®€å•åˆ†æé¡µé¢å†…å®¹
                from bs4 import BeautifulSoup
                soup = BeautifulSoup(html_content, 'html.parser')
                title = soup.title.string if soup.title else "æ— æ ‡é¢˜"
                print(f"   âœ… è¿æ¥æˆåŠŸ - å“åº”æ—¶é—´: {response_time}ms")
                print(f"      é¡µé¢æ ‡é¢˜: {title[:50]}...")
                print(f"      å†…å®¹é•¿åº¦: {len(html_content)} å­—ç¬¦")
                return True, response_time, len(html_content)
            else:
                print(f"   âŒ è¿æ¥å¤±è´¥ - æ— æ³•è·å–å†…å®¹")
                return False, response_time, 0
                
        except Exception as e:
            print(f"   âŒ è¿æ¥å¼‚å¸¸: {e}")
            return False, 0, 0

    def parse_simple_news(self, website, html_content):
        """ç®€å•è§£ææ–°é—»å†…å®¹ï¼ˆåŸºç¡€ç‰ˆæœ¬ï¼‰"""
        try:
            from bs4 import BeautifulSoup
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # å°è¯•æå–æ ‡é¢˜å’Œé“¾æ¥ï¼ˆé€šç”¨é€‰æ‹©å™¨ï¼‰
            articles = []
            
            # å¸¸è§çš„é€‰æ‹©å™¨æ¨¡å¼
            selectors = [
                'a[href*="article"]', 
                'a[href*="news"]',
                '.title a',
                '.news-item a',
                'h2 a',
                'h3 a'
            ]
            
            for selector in selectors:
                links = soup.select(selector)
                if links:
                    for link in links[:3]:  # åªå–å‰3ä¸ª
                        title = link.get_text().strip()
                        href = link.get('href', '')
                        
                        if title and len(title) > 10:
                            # å¤„ç†ç›¸å¯¹URL
                            if href.startswith('/'):
                                href = website['url'] + href
                            elif not href.startswith('http'):
                                href = website['url'] + '/' + href
                            
                            articles.append({
                                'title': title,
                                'url': href,
                                'source': website['name']
                            })
                    break
            
            return articles
            
        except Exception as e:
            print(f"   è§£æå¤±è´¥: {e}")
            return []

    def run_accessibility_test(self):
        """è¿è¡Œç½‘ç«™å¯è®¿é—®æ€§æµ‹è¯•"""
        print("ğŸŒ å¼€å§‹æµ‹è¯•åŒºå—é“¾æ–°é—»ç½‘ç«™å¯è®¿é—®æ€§...")
        print("=" * 60)
        
        results = []
        accessible_sites = []
        
        for website in self.websites:
            success, response_time, content_length = self.test_website_accessibility(website)
            
            results.append({
                'name': website['name'],
                'url': website['url'],
                'accessible': success,
                'response_time': response_time,
                'content_length': content_length
            })
            
            if success:
                accessible_sites.append(website['name'])
            
            # æ·»åŠ éšæœºå»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
            time.sleep(random.uniform(1, 3))
        
        # è¾“å‡ºæµ‹è¯•ç»“æœæ‘˜è¦
        print("\n" + "=" * 60)
        print("ğŸ“Š å¯è®¿é—®æ€§æµ‹è¯•ç»“æœæ‘˜è¦:")
        print(f"   æ€»æµ‹è¯•ç½‘ç«™: {len(self.websites)}")
        print(f"   å¯è®¿é—®ç½‘ç«™: {len(accessible_sites)}")
        print(f"   æˆåŠŸç‡: {len(accessible_sites)/len(self.websites)*100:.1f}%")
        
        if accessible_sites:
            print("   âœ… å¯è®¿é—®çš„ç½‘ç«™:")
            for site in accessible_sites:
                print(f"      - {site}")
        
        inaccessible = [r['name'] for r in results if not r['accessible']]
        if inaccessible:
            print("   âŒ ä¸å¯è®¿é—®çš„ç½‘ç«™:")
            for site in inaccessible:
                print(f"      - {site}")
        
        return results, accessible_sites

    def try_crawl_accessible_sites(self, accessible_sites):
        """å°è¯•æŠ“å–å¯è®¿é—®çš„ç½‘ç«™"""
        print("\nğŸ•·ï¸ å°è¯•æŠ“å–å¯è®¿é—®ç½‘ç«™çš„æ–°é—»...")
        
        crawled_articles = 0
        for website_info in self.websites:
            if website_info['name'] in accessible_sites:
                print(f"\nğŸ“° å°è¯•æŠ“å– {website_info['name']}...")
                
                try:
                    html_content = self.fetch_page(website_info['test_url'], timeout=15)
                    if html_content:
                        articles = self.parse_simple_news(website_info, html_content)
                        
                        if articles:
                            print(f"   å‘ç° {len(articles)} ç¯‡æ–‡ç« ")
                            
                            # ä¿å­˜åˆ°æ•°æ®åº“
                            for article in articles[:2]:  # åªä¿å­˜å‰2ç¯‡æµ‹è¯•
                                article_data = {
                                    'title': article['title'],
                                    'content': f"æ¥è‡ª {website_info['name']} çš„æ–°é—»: {article['title']}",
                                    'summary': f"æ¥è‡ª {website_info['name']}: {article['title']}",
                                    'source_name': website_info['name'],
                                    'source_url': website_info['url'],
                                    'source_type': 'news',
                                    'original_url': article['url'],
                                    'category': 'article',
                                    'tags': [website_info['name'], 'åŒºå—é“¾'],
                                    'publish_time': datetime.now(),
                                    'importance_score': 0.7
                                }
                                
                                if self.save_to_database(article_data):
                                    crawled_articles += 1
                                    print(f"   âœ… ä¿å­˜: {article['title'][:30]}...")
                        else:
                            print("   æœªæ‰¾åˆ°å¯è§£æçš„æ–‡ç« ")
                    else:
                        print("   æ— æ³•è·å–é¡µé¢å†…å®¹")
                        
                except Exception as e:
                    print(f"   æŠ“å–å¤±è´¥: {e}")
                
                # æ·»åŠ å»¶è¿Ÿ
                time.sleep(2)
        
        return crawled_articles

    def run(self):
        """è¿è¡Œå¤šæºçˆ¬è™«æµ‹è¯•"""
        print("ğŸš€ å¯åŠ¨å¤šæºåŒºå—é“¾æ–°é—»çˆ¬è™«æµ‹è¯•")
        
        # å¥åº·æ£€æŸ¥
        health = self.health_check()
        print(f"ğŸ“Š çˆ¬è™«å¥åº·çŠ¶æ€: {health}")
        
        # æµ‹è¯•ç½‘ç«™å¯è®¿é—®æ€§
        results, accessible_sites = self.run_accessibility_test()
        
        # å¦‚æœæœ‰å…³å¯è®¿é—®çš„ç½‘ç«™ï¼Œå°è¯•æŠ“å–
        if accessible_sites:
            crawled_count = self.try_crawl_accessible_sites(accessible_sites)
            print(f"\nğŸ‰ æŠ“å–å®Œæˆï¼ŒæˆåŠŸä¿å­˜ {crawched_count} ç¯‡æ–‡ç« ")
            return crawled_count
        else:
            print("\nğŸ˜ æ²¡æœ‰å¯è®¿é—®çš„ç½‘ç«™ï¼Œæ— æ³•æŠ“å–æ•°æ®")
            return 0
